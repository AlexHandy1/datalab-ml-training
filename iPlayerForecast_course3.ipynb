{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "## Quick reminder on last course\n",
    "\n",
    "XX\n",
    "\n",
    "## Goal of this course\n",
    "\n",
    "XX\n",
    "\n",
    "\n",
    "# Concepts\n",
    "\n",
    "## Training\n",
    "\n",
    "Notion of loss, cross-val (+for hyperparam)...\n",
    "\n",
    "What kind of model?\n",
    "\n",
    "## Model Selection\n",
    "\n",
    " \n",
    "The first decision you need to make in the model selection process is whether you plan to use a __classifier__ or a __regressor model__. Classifiers make discrete predictions about a datapoint into a finite number of classes while regressors make linear predictions.  \n",
    " \n",
    "Different models work in different ways and are more or less suitable for different problems. Fortunately, however,  understanding these specific differences is not essential to solve your data problems. The python module `scikit learn` contains all of the models that you are likely to need and the format of the data it requires is standardised across models. This makes it very easy to try your data using a myriad of different models and choose the one that performs best on your data.\n",
    " \n",
    "In our current project we use both classifiers and regressors to predict engagement. In the classifier we simply try to predict whether someone has watched any content in the two-week period while in the regressor we attempt to predict the number of minutes watched by the viewer within the two-week period.\n",
    " \n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "To evaluate our models there are various different approaches we can use. As previously mentioned, the final evaluation should ideally be using data that the model has never seen before (the training set). The training set is pushed through the trained model to make predictions and those predictions are compared with the actual targets.\n",
    "\n",
    "For classifiers we can use __the accuracy__ (percentage correct) or __the ROC curve__. When using the accuracy as a model metric it is important to bare in mind what the performance of a completely random model would be. This one depends on the average score. _DETAILS._ Any model you build must be evaluated in terms of improvement over this random model performance. The accuracy is the simplest metric but it gives us little insight into the behaviour of the model. ROC curves (and __the area under the curve__ statistic) give us a greater understanding of the separability of the data (https://en.wikipedia.org/wiki/Receiver_operating_characteristic).\n",
    "\n",
    "For regressors, the evaluation of the model is less straightforward and id usually based upon the average residual between the actual data and the model predictions. __Root mean squared error__ (RMSE) and __mean absolute error__ (MAE) are two commonly used examples of these. Once we have a representation of the error produced by the model we can compare that error with the error of the most simple model of the data (usually the mean). This statistic is known as the coefficient of determination or __R-squared__ and represents the part of variability the model managed to get.\n",
    "\n",
    "We won't tacke both prediction tasks here. This course focused on classification and the next one will focus on regression.\n",
    "\n",
    "# Classification\n",
    "\n",
    "The first thing to do is to get our data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put both training target arrays (regression and classification) in the same txt file\n",
    "# As both target arrays have the same size we just need to split it it two\n",
    "# and get the right part for the prediction task\n",
    "target_training = np.split(np.loadtxt('target_training'), 2)[1].flatten()\n",
    "features_training = pd.read_csv('features_training.csv')\n",
    "\n",
    "# Same for test data\n",
    "target_test = np.split(np.loadtxt('target_test'), 2)[1].flatten()\n",
    "features_test = pd.read_csv('features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User id as index\n",
    "features_training = features_training.set_index('user_id')\n",
    "features_test = features_test.set_index('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score\n",
    "\n",
    "Usually dealing with 0/1 classification problem we talk about __scoring__. And the probability to belong to the class 1 (usually our class of interest) is the score.\n",
    "\n",
    "As mentionned before we should have a baseline to compare the performance of our models with. We usually choose as a baseline score the one obtained by a random allocation of our users. Allocating randomly 100 users to the class 1, the accuracy will depend on the effective concentration of class 1 in the entire population. If we observe 30% of class 1 in the population, then we should have 30 correct predictions out of our 100 users with this random allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37152624614027346"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check our baseline score\n",
    "sum(target_training)/len(target_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for any classification model to add value we would like it to perform with an accuracy of more than 37% (otherwise guessing based on the proportions would be a better model).\n",
    "\n",
    "## Tree based classification model\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a simple tree based classification model\n",
    "from sklearn import tree\n",
    "\n",
    "# Accuracy as our error evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We will use cross validation, so import helper functions for this\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_max_depth=[2,3,4,6,8,10]\n",
    "param_min_leaf=[75,90,100,110,125,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: Max Depth: 4 - Min Sample Leaf: 90\n",
      "Score: 0.810101888901\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=[0,0]\n",
    "\n",
    "# We will use the itertools library to try all the possible combinations of paramaters\n",
    "# We could also have used the gridsearchCV capability in scikit learn\n",
    "for c in itertools.product(param_max_depth,param_min_leaf):\n",
    "    treeclass=tree.DecisionTreeClassifier(max_depth=c[0],min_samples_leaf=c[1])\n",
    "    scores=cross_val_score(treeclass,\n",
    "                           features_training,\n",
    "                           target_training,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=c\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: Max Depth:',best_param[0], '- Min Sample Leaf:',best_param[1])\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "treeclass=tree.DecisionTreeClassifier(max_depth=best_param[0],\n",
    "                                      min_samples_leaf=best_param[1])\n",
    "mod1=treeclass.fit(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81451257168063518"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1.score(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tw_lag1_watched</td>\n",
       "      <td>0.726536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>number_watched</td>\n",
       "      <td>0.163234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tw_lag2_watched</td>\n",
       "      <td>0.079183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tw_lag3_watched</td>\n",
       "      <td>0.021644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>num_weekday</td>\n",
       "      <td>0.005855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>average_completion</td>\n",
       "      <td>0.003547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tw_lag6_watched</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>most_weekday_weekday_4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>most_genre_Weather</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>most_weekday_weekday_0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature  importance\n",
       "5          tw_lag1_watched    0.726536\n",
       "9           number_watched    0.163234\n",
       "4          tw_lag2_watched    0.079183\n",
       "3          tw_lag3_watched    0.021644\n",
       "11             num_weekday    0.005855\n",
       "6       average_completion    0.003547\n",
       "0          tw_lag6_watched    0.000000\n",
       "28  most_weekday_weekday_4    0.000000\n",
       "23      most_genre_Weather    0.000000\n",
       "24  most_weekday_weekday_0    0.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp1=pd.DataFrame(\n",
    "    {'feature': list(features_training.columns),\n",
    "     'importance': list(mod1.feature_importances_)\n",
    "    })\n",
    "feature_imp1.sort_values(by='importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=mod1.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75647941060386792"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a random forrest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_max_depth=[2,3,4,6,8,10]\n",
    "param_min_leaf=[75,90,100,110,125,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: Max Depth: 8 - Min Sample Leaf: 75\n",
      "Score: 0.807455474643\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=[0,0]\n",
    "\n",
    "# We will use the itertools library to try all the possible combinations of paramaters\n",
    "# We could also have used the gridsearchCV capability in scikit learn\n",
    "for c in itertools.product(param_max_depth,param_min_leaf):\n",
    "    forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                        max_depth=c[0],min_samples_leaf=c[1])\n",
    "    scores=cross_val_score(forrestclass,\n",
    "                           features_training,\n",
    "                           target_training,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=c\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: Max Depth:',best_param[0], '- Min Sample Leaf:',best_param[1])\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                    max_depth=best_param[0],\n",
    "                                    min_samples_leaf=best_param[1])\n",
    "mod2=forrestclass.fit(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81451257168063518"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.score(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tw_lag1_watched</td>\n",
       "      <td>0.226734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>number_watched</td>\n",
       "      <td>0.132636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tw_lag2_watched</td>\n",
       "      <td>0.122547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>total_sessions</td>\n",
       "      <td>0.108742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>num_weekday</td>\n",
       "      <td>0.090680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>total_watched</td>\n",
       "      <td>0.084173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tw_lag3_watched</td>\n",
       "      <td>0.082247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tw_lag4_watched</td>\n",
       "      <td>0.040228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>num_genre</td>\n",
       "      <td>0.039364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>num_timeday</td>\n",
       "      <td>0.028694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature  importance\n",
       "5   tw_lag1_watched    0.226734\n",
       "9    number_watched    0.132636\n",
       "4   tw_lag2_watched    0.122547\n",
       "7    total_sessions    0.108742\n",
       "11      num_weekday    0.090680\n",
       "8     total_watched    0.084173\n",
       "3   tw_lag3_watched    0.082247\n",
       "2   tw_lag4_watched    0.040228\n",
       "10        num_genre    0.039364\n",
       "12      num_timeday    0.028694"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp2=pd.DataFrame(\n",
    "    {'feature': list(features_training.columns),\n",
    "     'importance': list(mod2.feature_importances_)\n",
    "    })\n",
    "feature_imp2.sort_values(by='importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred2=mod2.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7556900407841074"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try something that is not based on decision tree\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM tun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_C=[0.001,0.01,0.1,1.0,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: C: 10\n",
      "Score: 0.793009093676\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in param_C:\n",
    "    logclass=linear_model.LogisticRegression(C=i)\n",
    "    scores=cross_val_score(logclass,\n",
    "                           features_training,\n",
    "                           target_training,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: C:',best_param)\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "logclass=linear_model.LogisticRegression(C=best_param)\n",
    "mod3=logclass.fit(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79378032642258489"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod3.score(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        coef           feature\n",
      "11  0.312772       num_weekday\n",
      "22  0.147067  most_genre_Sport\n",
      "10  0.135839         num_genre\n",
      "12  0.128330       num_timeday\n",
      "9   0.009653    number_watched\n",
      "...\n",
      "        coef                 feature\n",
      "33 -0.335106    most_timeday_Morning\n",
      "13 -0.343705   most_genre_Children's\n",
      "31 -0.411556  most_timeday_Afternoon\n",
      "32 -0.438800    most_timeday_Evening\n",
      "34 -0.676191      most_timeday_Night\n"
     ]
    }
   ],
   "source": [
    "coef_mod3=pd.DataFrame(\n",
    "    {'feature': list(features_training.columns),\n",
    "     'coef': list(mod3.coef_.flatten())\n",
    "    })\n",
    "print(coef_mod3.sort_values(by='coef',ascending=False).head())\n",
    "print('...')\n",
    "print(coef_mod3.sort_values(by='coef',ascending=False).tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P VALUES ??\n",
    "AND which threshold ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3=mod3.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73819234311274828"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try to get some non linear patterns\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_C=[0.001,0.01,0.1,1.0,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: C: 1.0\n",
      "Score: 0.716144130385\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in param_C:\n",
    "    svcclass=svm.SVC(C=i)\n",
    "    scores=cross_val_score(svcclass,\n",
    "                           features_training,\n",
    "                           target_training,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: C:',best_param)\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "svcclass=svm.SVC(C=best_param)\n",
    "mod4=svcclass.fit(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95555800617556241"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod4.score(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred4=mod4.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69425075647941059"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, pred4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
