{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "## Quick reminder on last course\n",
    "\n",
    "XX\n",
    "\n",
    "## Goal of this course\n",
    "\n",
    "XX\n",
    "\n",
    "\n",
    "# Concepts\n",
    "\n",
    "## Training\n",
    "\n",
    "As explained in the previous course, we will build our model on a training set. In this dataset, as we are in the supervised framework, we observe both the features and the output variable (X,y). Each model will try here to relate the target output - what we want to forecast, to the features - what we can observe, using more or less complex formulas. For all algorithms we will actually minimize __the loss function__ - which quantifies how far our predictions on X will be with our model and its set of parameters to the correct output y. The estimation process can be tricky as we are considering complex relations between our features and output. Fortunately, Python is doing all the maths for us! If you want to learn more on this: __XX (SGD etc.)__\n",
    "\n",
    "In order to avoid overfitting - again - during the training part, the state of the art is to do __cross-validation__. Basically, instead of training our model on all the data, we will split the dataset in _n_ (usually 5) chunks. Train our model on four of it, test on the last one. \n",
    "\n",
    "cross-val (+for hyperparam)...\n",
    "\n",
    "What kind of model?\n",
    "\n",
    "## Model Selection\n",
    " \n",
    "The first decision you need to make in the model selection process in the supervised framework is whether you plan to use a __classifier__ or a __regressor model__. Classifiers make discrete predictions about a datapoint into a finite number of classes while regressors make linear predictions.  \n",
    " \n",
    "Different models work in different ways and are more or less suitable for different problems. Fortunately, however,  understanding these specific differences is not essential to solve your data problems. The python module `scikit learn` contains all of the models that you are likely to need and the format of the data it requires is standardised across models. This makes it very easy to try your data using a myriad of different models and choose the one that performs best on your data.\n",
    " \n",
    "In our current project we use both classifiers and regressors to predict engagement. In the classifier we simply try to predict whether someone has watched any content in the two-week period while in the regressor we attempt to predict the number of minutes watched by the viewer within the two-week period.\n",
    "\n",
    "We won't tacke both prediction tasks here. This course focused on classification and the next one will focus on regression.\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "To evaluate our models there are various different approaches we can use. As previously mentioned, the final evaluation should ideally be using data that the model has never seen before (the training set). The training set is pushed through the trained model to make predictions and those predictions are compared with the actual targets.\n",
    "\n",
    "For classifiers we can use __the accuracy__ (percentage correct) or __the ROC curve__. The accuracy is the simplest metric but it gives us little insight into the behaviour of the model. ROC curves (and __the area under the curve__ statistic) give us a greater understanding of the separability of the data. For more details see https://en.wikipedia.org/wiki/Receiver_operating_characteristic.\n",
    "\n",
    "When computing the performance of a model based on a given metric it's important to bare in mind what the performance of a simple model would be. Usually we consider the completely random one as a baseline. Any model you build must be evaluated in terms of improvement over this performance. \n",
    "\n",
    "# Classification\n",
    "\n",
    "The first thing to do is to get our data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put both training target arrays (regression and classification) in the same txt file\n",
    "# As both target arrays have the same size we just need to split it it two\n",
    "# and get the right part for the prediction task\n",
    "target_training = np.split(np.loadtxt('target_training'), 2)[1].flatten()\n",
    "features_training = pd.read_csv('features_training.csv')\n",
    "\n",
    "# Same for test data\n",
    "target_test = np.split(np.loadtxt('target_test'), 2)[1].flatten()\n",
    "features_test = pd.read_csv('features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User id as index\n",
    "features_training = features_training.set_index('user_id')\n",
    "features_test = features_test.set_index('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score\n",
    "\n",
    "Usually dealing with 0/1 classification problem we talk about __scoring__. And the probability to belong to the class 1 (usually our class of interest) is the score.\n",
    "\n",
    "As mentionned before we should have a baseline to compare the performance of our models with. We usually choose as a baseline score the one obtained by a random allocation of our users. Allocating randomly 100 users to the class 1, the accuracy will depend on the effective concentration of class 1 in the entire population. If we observe 30% of class 1 in the population, then we should have 30 correct predictions out of our 100 users labeled class 1 with this random allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37152624614027346"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check our baseline score\n",
    "sum(target_training)/len(target_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for any classification model to add value we would like it to perform with an accuracy of more than 37% (otherwise guessing based on the proportions would be a better model).\n",
    "\n",
    "## Tree based classification model\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a simple tree based classification model\n",
    "from sklearn import tree\n",
    "\n",
    "# Accuracy as our error evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We will use cross validation, so import helper functions for this\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_max_depth=[2,3,4,6,8,10]\n",
    "param_min_leaf=[75,90,100,110,125,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: Max Depth: 4 - Min Sample Leaf: 90\n",
      "Score: 0.810101888901\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=[0,0]\n",
    "\n",
    "# We will use the itertools library to try all the possible combinations of paramaters\n",
    "# We could also have used the gridsearchCV capability in scikit learn\n",
    "for c in itertools.product(param_max_depth,param_min_leaf):\n",
    "    treeclass=tree.DecisionTreeClassifier(max_depth=c[0],min_samples_leaf=c[1])\n",
    "    scores=cross_val_score(treeclass,\n",
    "                           features_training,\n",
    "                           target_training,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=c\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: Max Depth:',best_param[0], '- Min Sample Leaf:',best_param[1])\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "treeclass=tree.DecisionTreeClassifier(max_depth=best_param[0],\n",
    "                                      min_samples_leaf=best_param[1])\n",
    "mod1=treeclass.fit(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81451257168063518"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1.score(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tw_lag1_watched</td>\n",
       "      <td>0.726536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>number_watched</td>\n",
       "      <td>0.163234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tw_lag2_watched</td>\n",
       "      <td>0.079183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tw_lag3_watched</td>\n",
       "      <td>0.021644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>num_weekday</td>\n",
       "      <td>0.005855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>average_completion</td>\n",
       "      <td>0.003547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tw_lag6_watched</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>most_weekday_weekday_4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>most_genre_Weather</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>most_weekday_weekday_0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  importance\n",
       "0         tw_lag1_watched    0.726536\n",
       "1          number_watched    0.163234\n",
       "2         tw_lag2_watched    0.079183\n",
       "3         tw_lag3_watched    0.021644\n",
       "4             num_weekday    0.005855\n",
       "5      average_completion    0.003547\n",
       "6         tw_lag6_watched    0.000000\n",
       "7  most_weekday_weekday_4    0.000000\n",
       "8      most_genre_Weather    0.000000\n",
       "9  most_weekday_weekday_0    0.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp1=pd.DataFrame(\n",
    "    {'feature': list(features_training.columns),\n",
    "     'importance': list(mod1.feature_importances_)\n",
    "    })\n",
    "feature_imp1.sort_values(by='importance', ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=mod1.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75647941060386792"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a random forrest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_max_depth=[2,3,4,6,8,10]\n",
    "param_min_leaf=[75,90,100,110,125,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: Max Depth: 8 - Min Sample Leaf: 75\n",
      "Score: 0.807455474643\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=[0,0]\n",
    "\n",
    "# We will use the itertools library to try all the possible combinations of paramaters\n",
    "# We could also have used the gridsearchCV capability in scikit learn\n",
    "for c in itertools.product(param_max_depth,param_min_leaf):\n",
    "    forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                        max_depth=c[0],min_samples_leaf=c[1])\n",
    "    scores=cross_val_score(forrestclass,\n",
    "                           features_training,\n",
    "                           target_training,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=c\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: Max Depth:',best_param[0], '- Min Sample Leaf:',best_param[1])\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                    max_depth=best_param[0],\n",
    "                                    min_samples_leaf=best_param[1])\n",
    "mod2=forrestclass.fit(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81451257168063518"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.score(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tw_lag1_watched</td>\n",
       "      <td>0.226734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>number_watched</td>\n",
       "      <td>0.132636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tw_lag2_watched</td>\n",
       "      <td>0.122547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_sessions</td>\n",
       "      <td>0.108742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>num_weekday</td>\n",
       "      <td>0.090680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total_watched</td>\n",
       "      <td>0.084173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tw_lag3_watched</td>\n",
       "      <td>0.082247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tw_lag4_watched</td>\n",
       "      <td>0.040228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_genre</td>\n",
       "      <td>0.039364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num_timeday</td>\n",
       "      <td>0.028694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  importance\n",
       "0  tw_lag1_watched    0.226734\n",
       "1   number_watched    0.132636\n",
       "2  tw_lag2_watched    0.122547\n",
       "3   total_sessions    0.108742\n",
       "4      num_weekday    0.090680\n",
       "5    total_watched    0.084173\n",
       "6  tw_lag3_watched    0.082247\n",
       "7  tw_lag4_watched    0.040228\n",
       "8        num_genre    0.039364\n",
       "9      num_timeday    0.028694"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp2=pd.DataFrame(\n",
    "    {'feature': list(features_training.columns),\n",
    "     'importance': list(mod2.feature_importances_)\n",
    "    })\n",
    "feature_imp2.sort_values(by='importance', ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred2=mod2.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7556900407841074"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try something that is not based on decision tree\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM tun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_C=[0.001,0.01,0.1,1.0,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: C: 10\n",
      "Score: 0.793009093676\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in param_C:\n",
    "    logclass=linear_model.LogisticRegression(C=i)\n",
    "    scores=cross_val_score(logclass,\n",
    "                           features_training,\n",
    "                           target_training,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: C:',best_param)\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "logclass=linear_model.LogisticRegression(C=best_param)\n",
    "mod3=logclass.fit(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79378032642258489"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod3.score(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.312772</td>\n",
       "      <td>num_weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.147067</td>\n",
       "      <td>most_genre_Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.135839</td>\n",
       "      <td>num_genre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.128330</td>\n",
       "      <td>num_timeday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009653</td>\n",
       "      <td>number_watched</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef           feature\n",
       "0  0.312772       num_weekday\n",
       "1  0.147067  most_genre_Sport\n",
       "2  0.135839         num_genre\n",
       "3  0.128330       num_timeday\n",
       "4  0.009653    number_watched"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_mod3=pd.DataFrame(\n",
    "    {'feature': list(features_training.columns),\n",
    "     'coef': list(mod3.coef_.flatten())\n",
    "    })\n",
    "coef_mod3.sort_values(by='coef',ascending=False).reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.335106</td>\n",
       "      <td>most_timeday_Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.343705</td>\n",
       "      <td>most_genre_Children's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.411556</td>\n",
       "      <td>most_timeday_Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.438800</td>\n",
       "      <td>most_timeday_Evening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.676191</td>\n",
       "      <td>most_timeday_Night</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        coef                 feature\n",
       "30 -0.335106    most_timeday_Morning\n",
       "31 -0.343705   most_genre_Children's\n",
       "32 -0.411556  most_timeday_Afternoon\n",
       "33 -0.438800    most_timeday_Evening\n",
       "34 -0.676191      most_timeday_Night"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_mod3.sort_values(by='coef',ascending=False).reset_index(drop=True).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P VALUES ??\n",
    "AND which threshold ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3=mod3.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73819234311274828"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try to get some non linear patterns\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_C=[0.001,0.01,0.1,1.0,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: C: 1.0\n",
      "Score: 0.716144130385\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in param_C:\n",
    "    svcclass=svm.SVC(C=i)\n",
    "    scores=cross_val_score(svcclass,\n",
    "                           features_training,\n",
    "                           target_training,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: C:',best_param)\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "svcclass=svm.SVC(C=best_param)\n",
    "mod4=svcclass.fit(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95555800617556241"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod4.score(features_training,target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred4=mod4.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69425075647941059"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, pred4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
